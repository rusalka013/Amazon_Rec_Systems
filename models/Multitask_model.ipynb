{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multitask Recommender model "
      ],
      "metadata": {
        "id": "cWHF2-ZOkcaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a multi-task recommender with using both implicit (product clicks) and explicit signals (ratings). Ranking and Retrieval stages will be used. \n",
        "\n"
      ],
      "metadata": {
        "id": "SrSqld1bkgDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multitask Model"
      ],
      "metadata": {
        "id": "ZY5AZczlm_jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Klrwx7KllxBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOxGhpXPkPlI",
        "outputId": "c8768f44-5f08-472c-d144-ee648dfc9625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.49.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-recommenders\n",
        "! pip install -q --upgrade tensorflow-datasets\n",
        "! pip install -q scann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5GOFmmJk_kk",
        "outputId": "d709f392-64c6-4bd8-f42d-10cbd8dd2756"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 89 kB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 26.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 34.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 60.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "# import interactive table \n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "# set seed\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "hLsz6MVhlEez"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vNFTwEqlnJ1",
        "outputId": "d38975c1-3b07-4b91-f9b5-f2fd719586e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Dataset"
      ],
      "metadata": {
        "id": "ibZGvgLCl2xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data subset \n",
        "gdrive_path = '/content/drive/MyDrive/ModelingData'\n",
        "path = os.path.join(gdrive_path, \"ratings\")\n",
        "\n",
        "ratings = tf.data.Dataset.load(path)"
      ],
      "metadata": {
        "id": "pOMfPJ6zlUpA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    'product_title': x['data']['product_title'], \n",
        "    'customer_id': x['data']['customer_id'], \n",
        "    'star_rating': x['data']['star_rating']\n",
        "})\n",
        "products = ratings.map(lambda x: x['product_title'])"
      ],
      "metadata": {
        "id": "r5dluF84lcz2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(92_096, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(92_096)\n",
        "test = shuffled.skip(92_096).take(23_024)"
      ],
      "metadata": {
        "id": "RFHPwvnEmc9T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary to map raw feature values to embedding vectors\n",
        "product_titles = products.batch(50_000)\n",
        "customer_ids = ratings.batch(110_000).map(lambda x: x['customer_id'])\n",
        "\n",
        "unique_product_titles = np.unique(np.concatenate(list(product_titles)))\n",
        "unique_customer_ids = np.unique(np.concatenate(list(customer_ids)))"
      ],
      "metadata": {
        "id": "QX2U3MkGmkoU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the model"
      ],
      "metadata": {
        "id": "ljsUI92QnGfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two critical parts to multi-task recommenders:\n",
        "\n",
        "They optimize for two or more objectives, and so have two or more losses.\n",
        "They share variables between the tasks, allowing for transfer learning.\n",
        "In this tutorial, we will define our models as before, but instead of having a single task, we will have two tasks: one that predicts ratings, and one that predicts movie watches.\n",
        "\n",
        "The user and movie models are as before:"
      ],
      "metadata": {
        "id": "Iu0vnyfAnNsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 32\n",
        "\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_customer_ids, mask_token=None),\n",
        "  # We add 1 to account for the unknown token.\n",
        "  tf.keras.layers.Embedding(len(unique_customer_ids) + 1, embedding_dimension)\n",
        "])\n",
        "\n",
        "product_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_product_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_product_titles) + 1, embedding_dimension)\n",
        "])"
      ],
      "metadata": {
        "id": "EnZivPyYmy-i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will have two tasks: Ranking and Retrieval. "
      ],
      "metadata": {
        "id": "R3QbsP8hn2K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfrs.tasks.Ranking(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNOfKAvgny5f",
        "outputId": "10d7bb22-2ab6-4180-f469-e62f6cd37555"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.tasks.ranking.Ranking at 0x7f9044b7db50>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        candidates=products.batch(128)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIc2o4Uvn_o4",
        "outputId": "b71de8b1-e4a7-4d6d-ddd0-2ac0e58840a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.tasks.retrieval.Retrieval at 0x7f9044ce7a50>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
      ],
      "metadata": {
        "id": "1-UaLIH5pRxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a model\n",
        "class AmazonModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
        "    # We take the loss weights in the constructor: this allows us to instantiate\n",
        "    # several model objects with different loss weights.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # User and product models.\n",
        "    self.product_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_product_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_product_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_customer_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_customer_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # A small model to take in user and product embeddings and predict ratings.\n",
        "    # We can make this as complicated as we want as long as we output a scalar\n",
        "    # as our prediction.\n",
        "    self.rating_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    # The tasks.\n",
        "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=products.batch(128).map(self.product_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The loss weights.\n",
        "    self.rating_weight = rating_weight\n",
        "    self.retrieval_weight = retrieval_weight\n",
        "\n",
        "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"customer_id\"])\n",
        "    # And pick out the product features and pass them into the product model.\n",
        "    product_embeddings = self.product_model(features[\"product_title\"])\n",
        "\n",
        "    return (\n",
        "        user_embeddings,\n",
        "        product_embeddings,\n",
        "        # We apply the multi-layered rating model to a concatentation of\n",
        "        # user and product embeddings.\n",
        "        self.rating_model(\n",
        "            tf.concat([user_embeddings, product_embeddings], axis=1)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "\n",
        "    ratings = features.pop(\"star_rating\")\n",
        "\n",
        "    user_embeddings, product_embeddings, rating_predictions = self(features)\n",
        "\n",
        "    # We compute the loss for each task.\n",
        "    rating_loss = self.rating_task(\n",
        "        labels=ratings,\n",
        "        predictions=rating_predictions,\n",
        "    )\n",
        "    retrieval_loss = self.retrieval_task(user_embeddings, product_embeddings)\n",
        "\n",
        "    # And combine them using the loss weights.\n",
        "    return (self.rating_weight * rating_loss\n",
        "            + self.retrieval_weight * retrieval_loss)"
      ],
      "metadata": {
        "id": "wqC9vAjXoCzr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rating-specialized model"
      ],
      "metadata": {
        "id": "IIGcnGIUpaPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with model that only considers ratings. "
      ],
      "metadata": {
        "id": "V0jJoKovpfJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_model = AmazonModel(rating_weight=1.0, retrieval_weight=0.0)\n",
        "rating_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ],
      "metadata": {
        "id": "Xt3pihMPpbYG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle, batch, and cache train and test data\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "metadata": {
        "id": "Oi9aN7YfqGEn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_model.fit(cached_train, epochs=3)\n",
        "metrics = rating_model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpcBDG4qNoc",
        "outputId": "b8675254-f427-4362-920e-ba57812b272b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 1274s 105s/step - root_mean_squared_error: 2.4213 - factorized_top_k/top_1_categorical_accuracy: 1.0858e-05 - factorized_top_k/top_5_categorical_accuracy: 6.5149e-05 - factorized_top_k/top_10_categorical_accuracy: 7.6008e-05 - factorized_top_k/top_50_categorical_accuracy: 4.9948e-04 - factorized_top_k/top_100_categorical_accuracy: 9.3381e-04 - loss: 5.2712 - regularization_loss: 0.0000e+00 - total_loss: 5.2712\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 1284s 107s/step - root_mean_squared_error: 1.2111 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 6.5149e-05 - factorized_top_k/top_10_categorical_accuracy: 7.6008e-05 - factorized_top_k/top_50_categorical_accuracy: 5.1034e-04 - factorized_top_k/top_100_categorical_accuracy: 9.3381e-04 - loss: 1.4649 - regularization_loss: 0.0000e+00 - total_loss: 1.4649\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 1258s 104s/step - root_mean_squared_error: 1.2088 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 6.5149e-05 - factorized_top_k/top_10_categorical_accuracy: 7.6008e-05 - factorized_top_k/top_50_categorical_accuracy: 5.2120e-04 - factorized_top_k/top_100_categorical_accuracy: 9.4467e-04 - loss: 1.4599 - regularization_loss: 0.0000e+00 - total_loss: 1.4599\n",
            "6/6 [==============================] - 342s 55s/step - root_mean_squared_error: 1.2214 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 4.3433e-05 - factorized_top_k/top_10_categorical_accuracy: 8.6866e-05 - factorized_top_k/top_50_categorical_accuracy: 6.9493e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0013 - loss: 1.4959 - regularization_loss: 0.0000e+00 - total_loss: 1.4959\n",
            "Retrieval top-100 accuracy: 0.001.\n",
            "Ranking RMSE: 1.221.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieval-specialized model"
      ],
      "metadata": {
        "id": "7A0DuKn_xFSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_model = AmazonModel(rating_weight=0.0, retrieval_weight=1.0)\n",
        "retrieval_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ],
      "metadata": {
        "id": "zM8wvsx3xMrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_model.fit(cached_train, epochs=3)\n",
        "metrics = retrieval_model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ],
      "metadata": {
        "id": "0Mm5MXLUxcE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Joint model"
      ],
      "metadata": {
        "id": "64sOFaXnxix7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assigning equal weights to both tasks. "
      ],
      "metadata": {
        "id": "sifJYMgs669t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joint_model = AmazonModel(rating_weight=1.0, retrieval_weight=1.0)\n",
        "joint_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ],
      "metadata": {
        "id": "10nsWrVQxkwS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint_model.fit(cached_train, epochs=3)\n",
        "metrics = joint_model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3JxvmTCxtLp",
        "outputId": "74dccd20-8bc0-483b-94e3-7086078e0bb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 1294s 107s/step - root_mean_squared_error: 2.4679 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 2.1716e-05 - factorized_top_k/top_10_categorical_accuracy: 3.2575e-05 - factorized_top_k/top_50_categorical_accuracy: 1.3030e-04 - factorized_top_k/top_100_categorical_accuracy: 3.1489e-04 - loss: 64784.0261 - regularization_loss: 0.0000e+00 - total_loss: 64784.0261\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 1226s 101s/step - root_mean_squared_error: 1.2099 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 2.1716e-05 - factorized_top_k/top_10_categorical_accuracy: 3.2575e-05 - factorized_top_k/top_50_categorical_accuracy: 0.0065 - factorized_top_k/top_100_categorical_accuracy: 0.0189 - loss: 64580.3278 - regularization_loss: 0.0000e+00 - total_loss: 64580.3278\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 1222s 101s/step - root_mean_squared_error: 1.2101 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0472 - factorized_top_k/top_100_categorical_accuracy: 0.0906 - loss: 63743.5092 - regularization_loss: 0.0000e+00 - total_loss: 63743.5092\n",
            "6/6 [==============================] - 322s 53s/step - root_mean_squared_error: 1.2220 - factorized_top_k/top_1_categorical_accuracy: 6.9493e-04 - factorized_top_k/top_5_categorical_accuracy: 6.9493e-04 - factorized_top_k/top_10_categorical_accuracy: 6.9493e-04 - factorized_top_k/top_50_categorical_accuracy: 7.3836e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0011 - loss: 30037.9933 - regularization_loss: 0.0000e+00 - total_loss: 30037.9933\n",
            "Retrieval top-100 accuracy: 0.001.\n",
            "Ranking RMSE: 1.222.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very low accuracy rate at 0.1% for Top-100 recs on test data. "
      ],
      "metadata": {
        "id": "K-AoltMqLuuX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkKPIxU5L8A3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}